\section{Approach}
\begin{itemize}
    \item \textbf{Dataset:} Use available automotive sensor and telemetry data supporting tasks such as predictive maintenance and anomaly detection.
    \item \textbf{Phase 1 – Baseline:} Train ML models on uncompressed and event-based data to quantify loss in predictive utility.
    \item \textbf{Phase 2 – Neural compression:} Implement autoencoder and VAE baselines for sensor streams \citep{bhattacharya2022, ravi2024}.
    \item \textbf{Phase 3 – Tokenization framework:}
    \begin{itemize}
        \item Develop a learnable tokenization module that discretizes data into semantically meaningful units optimized for downstream tasks \citep{tokenization2024}.
        \item Train end-to-end with joint objective: 
        \[
        \mathcal{L} = \lambda \cdot \text{Rate} + (1 - \lambda) \cdot \text{Task Loss}
        \]
        following a rate–utility perspective \citep{dataset_distillation2025}.
        \item Explore adaptive token sizes based on model uncertainty \citep{wiessner2024}.
    \end{itemize}
    \item \textbf{Phase 4 – Evaluation:}
    \begin{itemize}
        \item Measure rate–utility curves across methods.
        \item Compare tokenization, neural compression, and event-triggered baselines.
        \item Evaluate trade-offs between bandwidth and ML accuracy.
    \end{itemize}
    \item \textbf{Expected Outcome:} Demonstrate that task-aware tokenization achieves efficient compression without significant ML degradation, offering a foundation for adaptive telemetry and co-design in future vehicles.
\end{itemize}