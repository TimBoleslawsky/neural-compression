\section{Problem}\label{sec:problem}

% Introduction of Problem - Done
Constructing downstream ML models for automotive systems, or in fact Internet-of-Things (IoT) systems in general, is a constant trade-off between compression large quantities of data and maximizing model performance \citep{cuza2024}. Traditional compression techniques can reduce data volume, but often at the cost of losing critical information necessary for accurate ML tasks such as predictive maintenance, anomaly detection, and fleet analytics. The impact of this trade-off is well-documented in the literature.~\citep{cuza2024} for example study the impact of lossy compression techniques on time series forecasting tasks and observe a constant trade-off between compression ratio and forecasting accuracy. Another approach to reducing the data volume is event based logging which offers a very lightweight solution for the problem but similarly risks loosing useful data. % Not sure we should mention event based logging here?

% Examples
The use of task-aware compression modules or neural compression are two potential solutions to this problem which are believed to have room for further exploration as explored in literature surveys such as \cite{liu2023machine}. Here it is mentioned that the use of machine learning for producing compressed representations of the data on edge devices is largely underexplored due to the high cost of these models and the restraints of embedded systems. This presents an interesting gap in the literature for further exploration. 

Neural compression models leverage deep learning techniques to learn efficient data representations to compress data \citep{yang2022}. Studies as early as 2019 have shown that neural compression methods can outperform traditional compression techniques for image and video data, especially at low bitrates \citep{löhdefink2019}. The same has been shown for time series data \citep{zheng2023}. % Maybe go into detail here on the differences between representation learning and neural compression

Task-aware compression techniques, on the other hand, focus on optimizing compression algorithms to retain information that is most relevant for specific ML tasks \citep{}. This idea has shown promise in handling time-series data more efficiently in IoT systems.~\citep{azar2022robust} and \citep{sun2025} for example explore task-aware compression algorithms that adaptively prioritize data features based on their relevance to downstream tasks, demonstrating improved performance in resource-constrained environments.

When combining these two techniques task-aware neural compression models, have shown promise in reducing the loss of utility associated with higher compression ratios, commonly referred to as the rate-utility trade-off. These models are specifically designed to retain essential features for ML tasks while achieving high compression ratios \citep{yang2022}. Studies such as \citep{kawawabeaudan2022} and \cite{10.1145/3534678.3539329} have empirically evaluated task-aware neural compression models and shown that this approach have potential for optimizing two parameters. In \citep{kawawabeaudan2022} they use a hierarchical autoencoder-based compression network together with a recognition model and implement two hyperparameters to trade off between distortion, bitrate, and recognition performance, while in \cite{10.1145/3534678.3539329} the ability to reconstruct the to perform prediction tasks. % Added tarnet paper

% Two major challenges
There are two major limitations with the examples discussed above. First, while there exist some exploration of task-aware neural compression techniques for image and video data \citep{kawawabeaudan2022}, there is a notable lack of research focusing on time series data, which is the predominant data type in automotive and IoT applications.

The second limitation, that most of these papers fail to address, is the computational constraints of in-vehicle embedded or IoT systems. The mentioned papers, if they use neural compression, primarily focus on achieving high compression rates while maintaining model performance. Because of this, computationally heavy neural network architectures like recurrent neural networks (RNNs) or transformers were chosen \citep{zheng2023, löhdefink2019, kawawabeaudan2022}.

% Conclusion - Done
So while modern task-aware compression techniques, like neural compression, have shown promising advancements in balancing the compression and model performance trade-off, there remains a significant gap in systematically understanding and optimizing the rate-utility trade-off, specifically in vehicular contexts, where computational resources and bandwidth are often constrained.




