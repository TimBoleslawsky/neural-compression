\section{Approach}\label{sec:approach}

% Justification of tokenization as a neural compression strategy - citations are TBD
As introduced in Section~\ref{sec:context}, neural compression techniques leverage deep learning to enhance data distribution capabilities as well as enable learned transform coding and quantization \citep{yang2022}. It has also been discussed, how neural compression methods often rely on computationally heavy architectures like RNNs and transformers \citep{zheng2023, l√∂hdefink2019, kawawabeaudan2022, liu2024deepdictdeeplearningbased}, to solve the task of entropy modeling within the compression pipeline. Continuous latents have dominated historically because they are easier to optimize end-to-end with gradient descent, whereas discrete latent learning used to be unstable or difficult \citep{tobecited}.

An emerging idea is to use discrete latent representations within the transform step of the compression pipeline, which simplify the entropy modeling task and therefore enable more efficient compression. This idea is the main inspiration behind the Vector Quantised-Variational AutoEncoder (VQ-VAE) architecture \citep{vandenoord2017neural}, which proposes the use of vector quantization as a way to learn discrete latent spaces. While the VQ-VAE architecture and its successors have been successfully applied to image and audio data, their main focus remains reconstruction \citep{vandenoord2017neural, razavi2019generating}. This makes them not ideally suited for task-aware compression. Tokenization emerges as an alternative approach in audio and speech processing research \citep{schmidt2024tokenization}. 

Tokenization is traditionally understood as the mapping of high-dimensional, continuous inputs into a sequence of discrete symbols drawn from a finite vocabulary \citep{grefenstette1999}. Tokenization therefore can act as a form of transformation and quantization: it reduces dimensionality, decorrelates, and constrains representations to a compact code space. Additionally, tokenization can be made task-aware so that the retained tokens are maximally useful for prediction or classification. One example of this is the WavTokenizer, which efficiently tokenizes acoustic data for audio language modeling \citep{ji2025}. We propose, that this idea can be translated to time series data to enable lightweight entropy modeling architectures. This would allow more computationally efficient compression pipelines, which, as shown, is especially relevant for in-vehicle embedded systems with limited computational resources.

% Proposed Approach - decide on a baseline and formulate
\begin{itemize}
    \item \textbf{Dataset:} Use available automotive sensor and telemetry test-fleet data supporting tasks such as predictive maintenance and anomaly detection.
    \item \textbf{Task 1:} Train downstream ML models on uncompressed data to quantify loss in predictive utility.
    \item \textbf{Task 2:} Implement baseline. % What to use?
    \item \textbf{Task 3:} Develop a learnable tokenization module that discretizes data into semantically meaningful units optimized for downstream tasks.
    \item \textbf{Task 4:} Develop lightweight entropy modeling and coding schemes tailored to the tokenized representations.
    \item \textbf{Task 5:} Evaluate and compare the methods.
    \begin{itemize}
        \item Measure rate-utility curves across the methods.
        \item Evaluate trade-offs between computational efficiency.
    \end{itemize}
    \item \textbf{Expected Outcome:} Demonstrate that task-aware tokenization achieves comparable rate-utility trade-off to established neural compression approaches, while increasing computational efficiency. 
\end{itemize}